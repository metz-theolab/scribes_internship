{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This are the dependencies required for the project\n",
    "\n",
    "!pip install collatex\n",
    "!pip install graphviz\n",
    "!pip install levenshtein\n",
    "!pip install xmltodict\n",
    "!pip install beautifulsoup4\n",
    "!pip install lxml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This markdown portion is to introduce you the the current implementation details.\n",
    "\n",
    "The code below assumes that the pre-processed XML to be available in the same folder as of the .ipynb file running from.\n",
    "This is something that can be fixed in the near future.\n",
    "\n",
    "The current implementation details are as follows:\n",
    "\n",
    "First Part:\n",
    "It parses the XML files to extract the chapter information and the verse number information. Once the data is arranged we generate the witness\n",
    "\n",
    "Second Part:\n",
    "The verses are arranged as an input to the collatex tool for further analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if beautiful soup can be of any use\n",
    "# This part of the code is clunky but it is what it is\n",
    "\n",
    "# Implementation of the first part\n",
    "from itertools import combinations\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "fileNames = [\"ms_a_new.xml\",\"ms_b_new.xml\",\"ms_d_new.xml\",\"ms_e_new.xml\",\"ms_f_new.xml\"]\n",
    "soup = dict()\n",
    "chapter = dict()\n",
    "verse_list = dict()\n",
    "verses = dict()\n",
    "\n",
    "for currFile in fileNames:\n",
    "    with open(currFile,encoding=\"utf8\") as fp:\n",
    "        soup[currFile] = BeautifulSoup(fp,features='xml')\n",
    "        chs = list()\n",
    "        verse_list[currFile] = dict()\n",
    "        verses[currFile] =dict()\n",
    "        for i in soup[currFile].findAll(\"chap\"):\n",
    "            ch_string=i.contents[0].strip()\n",
    "            chs.append(ch_string) # strip to remove trailing spaces or new line characters\n",
    "            verses_list = list()\n",
    "            verse = i.findAll(\"text\")\n",
    "            verses[currFile][ch_string] = dict()\n",
    "            for verse_iter in verse:\n",
    "                if verse_iter.verse_nb:\n",
    "                    verse_num=verse_iter.verse_nb.text.strip()\n",
    "                verses_list.append(verse_num)\n",
    "                \"\"\"\n",
    "                <!ELEMENT folio (#PCDATA)> <!-- shelfmark of the manuscript and folio number -->\n",
    "                <!ELEMENT verse_nb (#PCDATA)> <!-- verse (children of chapter) -->\n",
    "                <!ELEMENT line (#PCDATA)> <!-- line on the manuscript -->\n",
    "                <!ELEMENT vacat_car (#PCDATA)> <!-- a space into the manuscript -->\n",
    "                <!ELEMENT greek (#PCDATA)> <!-- greek word or letter -->\n",
    "                <!ELEMENT reconstructed (#PCDATA)> <!-- Hebrew reconstructed -->\n",
    "                <!ELEMENT superscript (#PCDATA)> <!-- Hebrew superscript letters or words -->\n",
    "                <!ELEMENT supralinear (#PCDATA)> <!-- Hebrew supralinear letters or words (I think = superscript) -->\n",
    "                <!ELEMENT margin_reconstructed (#PCDATA)> <!-- marginal notation reconstructed -->\n",
    "                <!ELEMENT margin_car (#PCDATA)> <!-- marginal notation -->\n",
    "                <!ELEMENT margin_infralinear (#PCDATA)> <!-- marginal notation -->\n",
    "                <!ELEMENT margin_supralinear (#PCDATA)> <!-- marginal notation -->\n",
    "                \"\"\"\n",
    "                # For now clean the text which might be enclosed in the tags\n",
    "                unwanted_tags= [\"folio\",\"verse_nb\",\"line\",\"vacat_car\",\"greek\",\"reconstructed\",\"superscript\",\\\n",
    "                \"supralinear\",\"margin_reconstructed\",\"margin_car\",\"margin_infralinear\",\\\n",
    "                \"margin_supralinear\",\"Article\"]\n",
    "                \n",
    "                # Clean tags\n",
    "                [s.extract() for s in verse_iter(unwanted_tags)]\n",
    "                \n",
    "                # FileName will never be empty\n",
    "                if (ch_string and verse_num):\n",
    "                    verses[currFile][ch_string][verse_num] = verse_iter.text.replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "            verse_list[currFile][ch_string]=verses_list\n",
    "        \n",
    "        chapter[currFile] = chs\n",
    "        #verse = chapter[currFile].findAll(\"text\")\n",
    "        #for i in verse:\n",
    "            #print(i.verse_nb.text)\n",
    "            #print(i.contents)\n",
    "            #ls = [type(item) for item in i.contents]\n",
    "            #print(ls)\n",
    "            #print(\"\\n\")\n",
    "\n",
    "\n",
    "#print(soup['ms_d_new.xml'].findAll(\"chap\")[0].contents[0])\n",
    "\n",
    "\n",
    "\n",
    "# Implementation of the second part\n",
    "# Building the combinations\n",
    "chap_matching = dict()\n",
    "witnesses = dict()\n",
    "chap_info = dict()\n",
    "count = 0\n",
    "for combo in combinations(fileNames, 2):  # 2 for pairs, 3 for triplets, etc\n",
    "    matching_chs_list = []\n",
    "    if combo[0] in chap_matching.keys():\n",
    "        pass\n",
    "    else:\n",
    "        chap_matching[combo[0]] = dict()\n",
    "    for chap_in_file1 in chapter[combo[0]]:\n",
    "        for chap_in_file2 in chapter[combo[1]]:\n",
    "            ch1_num=re.findall(r'\\b\\d+\\b', chap_in_file1)\n",
    "            ch2_num=re.findall(r'\\b\\d+\\b', chap_in_file2)\n",
    "            if ch1_num == ch2_num:\n",
    "                matching_chs_list.append([chap_in_file1,chap_in_file2]) \n",
    "                # Lets see if we can find common verses\n",
    "                common_verses= set(verse_list[combo[0]][chap_in_file1]).intersection(verse_list[combo[1]][chap_in_file2])\n",
    "                for com_verse in common_verses:\n",
    "                    if com_verse:\n",
    "                        witnesses[count] = dict()\n",
    "                        witnesses[count]['A'] = verses[combo[0]][chap_in_file1][com_verse]\n",
    "                        witnesses[count]['B'] = verses[combo[1]][chap_in_file2][com_verse]\n",
    "                        chap_info[count] = (combo[0],chap_in_file1,combo[1],chap_in_file2,com_verse)\n",
    "                        count = count+1;\n",
    "    chap_matching[combo[0]][combo[1]]=matching_chs_list\n",
    "    \n",
    "\n",
    "# Changing the witness variable here will change the verse which will be loaded for comaparison\n",
    "witness_index=22\n",
    "from collatex import *\n",
    "collation = Collation()\n",
    "collation.add_plain_witness(\"A\", witnesses[witness_index]['A'])\n",
    "collation.add_plain_witness(\"B\", witnesses[witness_index]['B'])\n",
    "print(\"\\nManuscript A: \"+witnesses[witness_index]['A'])\n",
    "print(\"Manuscript B: \"+witnesses[witness_index]['B']+\"\\n\")\n",
    "#alignment_table = collate(collation, segmentation=False)\n",
    "#print(chap_matching)\n",
    "#print(verses)\n",
    "print(chap_info[witness_index])\n",
    "alignment_table = collate(collation,output=\"svg\")\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code is for the pre-processing part of the XML files to generate the ms_a_new.xml , ms_b_new.xml.\n",
    "\n",
    "This is not the most efficiently way of implementing it, but this is for proof of concept which can be eventually made into a finer module\n",
    "\n",
    "The code below is not automatic. The input and output filenames needs to be specified manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is building an alternate parser for the XML\n",
    "# This block is meant for fixing the tags of the <chap>\n",
    "# This block is for fixing the incorrect tag closing of <chap>\n",
    "from lxml import etree\n",
    "import json,re,xmltodict\n",
    "\n",
    "\n",
    "# First load a document and find the number of chapters the file contains\n",
    "with open(\"../data/ms_f.xml\",encoding=\"utf8\") as xml_file:\n",
    "\n",
    "    #read content of file to string\n",
    "    data = xml_file.read()\n",
    "    #first replace </chap> with empty\n",
    "    new_data=data.replace(\"</chap>\",\"\")\n",
    "    #Then replace <chap> with </chap><chap>\n",
    "    #Replace first occurrence to restore <chap>\n",
    "    new_data1=new_data.replace(\"<chap>\",\"</chap><chap>\").replace(\"</chap><chap>\",\"<chap>\",1)\n",
    "    # To compensate chapter closing replace </Article> with </chap></Article>\n",
    "    # But this has to be the last occurence so we use rfind\n",
    "    replacementStr=\"</chap></Article>\"\n",
    "    new_data1=replacementStr.join(new_data1.rsplit(\"</Article>\", 1))\n",
    "\n",
    "    #json_data = json.dumps(final_data,ensure_ascii=False,indent=1)\n",
    "     \n",
    "    # Write the json data to output\n",
    "    # json file\n",
    "    with open(\"ms_f_new.xml\", \"w\", encoding=\"utf8\") as json_file:\n",
    "        json_file.write(new_data1)\n",
    "        json_file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "649cc9d069e96972753a67e62a2602ce75c07dd6e0b075b893690b6a9559d424"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
